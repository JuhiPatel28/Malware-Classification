# -*- coding: utf-8 -*-
"""RF_Important_Features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_F4Z-r_PywAJt1feEpsUist6-DlzESOl

Importing necessary libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

"""Reading the dataset with Pandas after mounting the dataset in google colab"""

#reading the dataset with a dataframe
df = pd.read_csv("final_preprocessed_dataset.csv")
df=df.drop('Unnamed: 0',1)

#eye balling the data
df.head(5)

"""Dropping resource column and categorical APT group column"""

df = df.drop('Resource', 1)
df =df.drop('APT Group',1)

#eye balling information about our dataset
df.head(5)
df.info()

"""Sklearn Train Test Split Method : Splitting the data into training and testing data."""

#the labelled column is Apt Group and the rest is features
label = np.array(df['APTGroup'])
features = df.drop('APTGroup',1)

features.head()

label

#choosing a 70-30 split to test out the performance
from sklearn.model_selection import train_test_split
seed =50
X_train, X_test, y_train, y_test = train_test_split(features,label,test_size=0.30, random_state = seed)

X_train.head(5)

X_test.head(5)

"""Building the Random Forest Classifier"""

rf_classifier = RandomForestClassifier(
                      min_samples_leaf=50,
                      n_estimators=150,
                      bootstrap=True,
                      oob_score=True,
                      n_jobs=-1,
                      random_state=seed,
                      max_features='auto')

rf_classifier.fit(X_train,y_train)

y_pred = rf_classifier.predict(X_test)

print(y_pred)

"""Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""# Observation:

Accuracy acquired about 58% accuracy. Needs more hyperparameter tuning to increase the accuracy. Maybe contributing other factors along the way too can lead to better accuracy.

## Model Attempt#2 : Hyper Parameter tunning
"""

"""
rf_classifier = RandomForestClassifier(
                      min_samples_leaf=50,
                      n_estimators=150,
                      bootstrap=True,
                      oob_score=True,
                      n_jobs=-1,
                      random_state=seed,
                      max_features='auto')
                      """
rf_classifier = RandomForestClassifier(
                      min_samples_leaf=3,
                      n_estimators=300,
                      bootstrap=True,
                      oob_score=True,
                      n_jobs=-1,
                      random_state=seed,
                      max_features='auto')

rf_classifier.fit(X_train, y_train)
y_pred = rf_classifier.predict(X_test)
print("accuracy ", accuracy_score(y_test, y_pred) )

"""#Cross validation """

df.head(5)

from sklearn.model_selection import ShuffleSplit
 ss = ShuffleSplit(n_splits=20, test_size=0.30)
accuracy=0
for train_indices, test_indices in ss.split(df):
  #print("Train")
  #print(X_train)
  #print("Test")
  #print(X_test)
  
  train = df.iloc[train_indices]
  y_train= np.array(train['APTGroup'])
  X_train= train.drop('APTGroup',1)
  

  test = df.iloc[test_indices]
  y_test= np.array(train['APTGroup'])
  X_test= train.drop('APTGroup',1)
  
  rf_classifier.fit(X_train, y_train)
  y_pred = rf_classifier.predict(X_test)
  print("accuracy ", accuracy_score(y_test, y_pred) )
  accuracy+=accuracy_score(y_test, y_pred)

print(accuracy/20)

"""# Observation 1:

Accuracy acquired : 86.62 % for n_splits=20 \\

# Another method for cross validation: cross_val_score
"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
print(cross_val_score(rf_classifier, features, label, cv=20, scoring ='accuracy').mean())

"""# Observation 2:

Accuracy acquired : 84.2 % for n_splits=20 \\

## Important Feature Calculation
"""

rf_classifier.fit(X_train,y_train)

df.head(3)

pd.set_option("display.max.rows", None)

# Important Features
feature_importance = pd.DataFrame(rf_classifier.feature_importances_,
                                   index = X_train.columns,
                                    columns=['importance']).sort_values('importance',ascending=False)
feature_importance
# summarize feature importance
#for i,v in enumerate(importance):
 #   print('Feature: %0d, Score: %.5f' % (i,v))

"""Exporting the results into an excel file for better view"""

excel_data = feature_importance.copy()

excel_data.to_csv('important_features.csv', index=True)

# Commented out IPython magic to ensure Python compatibility.
# %ls

#checking with shell command if there was a proper write in the new file created
! cat important_features.csv